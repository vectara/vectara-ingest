# S3 Crawler

The S3 crawler indexes files from AWS S3 buckets with support for file filtering, parallel processing using Ray, metadata extraction from CSV files, and flexible bucket access configurations. It handles various file types including documents (PDF, DOCX), spreadsheets, audio/video files, and custom S3-compatible endpoints.

## Overview

- **Crawler Type**: `s3`
- **Authentication**: AWS credentials (access key and secret key)
- **Endpoints**: AWS S3 or S3-compatible services (MinIO, DigitalOcean Spaces, etc.)
- **File Types**: All formats (PDF, DOCX, XLSX, CSV, images, audio, video, etc.)
- **Metadata**: Automatic S3 object metadata, optional custom metadata from CSV
- **Parallel Processing**: Ray-based distributed processing with configurable workers
- **Rate Limiting**: Configurable requests per second
- **SSL Verification**: Flexible SSL verification (disable, custom certificate path, or default)

## Use Cases

- Index documents stored in AWS S3 buckets
- Build searchable knowledge base from cloud file storage
- Process multiple file types from S3 at scale
- Integrate with data lakes and cloud storage solutions
- Archive and index historical documents from S3
- Parallel processing of large S3 buckets
- Mix cloud storage with metadata enrichment
- Integrate with S3-compatible storage systems (MinIO, Spaces)
- Automatic media file indexing with transcription
- Enterprise document management from cloud storage

## Getting Started: AWS Credentials Setup

### Prerequisites

- AWS Account with S3 bucket access
- AWS Access Key ID and Secret Access Key
- Bucket name and path (prefix) where files are stored
- Appropriate IAM permissions on the bucket

### Getting AWS Credentials

1. **Sign into AWS Console**:
   - Go to [AWS IAM Console](https://console.aws.amazon.com/iam/)

2. **Create or Use Existing User**:
   - Click "Users" in the sidebar
   - Select existing user or create new one
   - Click "Create access key" (or look for existing keys under Security credentials tab)

3. **Copy the Credentials**:
   - Note your `Access Key ID`
   - Note your `Secret Access Key` (visible only once!)
   - Save these securely

### Required IAM Permissions

The IAM user or role needs these minimum S3 permissions:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:ListBucket",
        "s3:GetObject"
      ],
      "Resource": [
        "arn:aws:s3:::your-bucket-name",
        "arn:aws:s3:::your-bucket-name/*"
      ]
    }
  ]
}
```

**Permission breakdown**:
- `s3:ListBucket` - List objects in the bucket and list objects by prefix
- `s3:GetObject` - Download individual files from the bucket

### Setting Environment Variables

Store credentials securely using environment variables:

```bash
export AWS_ACCESS_KEY_ID="AKIAIOSFODNN7EXAMPLE"
export AWS_SECRET_ACCESS_KEY="wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
```

**Security Note**: Never hardcode AWS credentials in configuration files. Always use environment variables or AWS credential files.

### AWS Credential File (Alternative)

For local development, use `~/.aws/credentials`:

```ini
[default]
aws_access_key_id = AKIAIOSFODNN7EXAMPLE
aws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY

[another-profile]
aws_access_key_id = AKIAIOSFODNN7ANOTHER
aws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY2
```

## Configuration

### Basic Configuration

```yaml
vectara:
  endpoint: api.vectara.io
  corpus_key: s3-documents

crawling:
  crawler_type: s3

s3_crawler:
  # S3 path in format: s3://bucket-name/prefix
  s3_path: "s3://my-bucket/documents"

  # AWS credentials (use environment variables recommended)
  aws_access_key_id: "${AWS_ACCESS_KEY_ID}"
  aws_secret_access_key: "${AWS_SECRET_ACCESS_KEY}"

  # File extensions to include
  extensions: [".pdf", ".docx"]
```

### Advanced Configuration

```yaml
vectara:
  endpoint: api.vectara.io
  corpus_key: s3-knowledge-base
  reindex: false
  verbose: true

  chunking_strategy: sentence
  chunk_size: 512
  remove_code: false

crawling:
  crawler_type: s3

s3_crawler:
  # S3 bucket and prefix path
  s3_path: "s3://enterprise-docs/knowledge-base"

  # AWS credentials
  aws_access_key_id: "${AWS_ACCESS_KEY_ID}"
  aws_secret_access_key: "${AWS_SECRET_ACCESS_KEY}"

  # File filtering
  extensions: [".pdf", ".docx", ".xlsx", ".csv"]

  # Optional metadata CSV file in S3
  metadata_file: "metadata.csv"

  # Source identifier for tracking
  source: "s3-enterprise"

  # Parallel processing with Ray
  ray_workers: 4              # Number of parallel workers (0 = sequential, -1 = CPU count)
  num_per_second: 10          # Rate limiting: files per second

  # S3-compatible endpoint (optional)
  endpoint_url: null          # Set for MinIO, DigitalOcean Spaces, etc.

doc_processing:
  # Document parsing options
  doc_parser: unstructured
  parse_tables: true
  do_ocr: false               # Enable for scanned PDFs (slower)
  summarize_images: false

metadata:
  source: s3
  environment: production
  collection_type: documents
  category: knowledge-base
```

## Configuration Parameters

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `s3_path` | string | Yes | - | S3 path in format `s3://bucket-name/prefix` |
| `aws_access_key_id` | string | Yes | - | AWS Access Key ID (use environment variable) |
| `aws_secret_access_key` | string | Yes | - | AWS Secret Access Key (use environment variable) |
| `extensions` | list | Yes | - | File extensions: `["*"]` for all or specific like `[".pdf", ".docx"]` |
| `metadata_file` | string | No | None | Path to CSV metadata file in S3 (relative to bucket root) |
| `source` | string | No | `"S3"` | Source label for metadata tracking |
| `ray_workers` | int | No | `0` | Parallel workers: 0 = sequential, -1 = auto (CPU count), N > 0 = N workers |
| `num_per_second` | int | No | `10` | Rate limiting (minimum 1) - files downloaded per second |
| `endpoint_url` | string | No | None | Custom S3 endpoint URL (for MinIO, DigitalOcean Spaces, etc.) |

### SSL/TLS Configuration

Control SSL certificate verification through the vectara section:

```yaml
vectara:
  # Default: Use system certificates
  ssl_verify: true

  # Disable SSL verification (not recommended for production)
  ssl_verify: false

  # Custom certificate file path
  ssl_verify: "/path/to/ca-bundle.crt"
  # Or with ~ expansion:
  ssl_verify: "~/certificates/custom-ca.crt"
```

**SSL Verification Options**:
- `true` - Use default system certificates (recommended)
- `false` - Disable verification (only for testing/development)
- `"/path/to/cert.crt"` - Use custom certificate file

## S3 Path Format

The `s3_path` parameter follows the standard S3 URI format:

### Format: `s3://bucket-name/prefix`

```yaml
# Root of bucket
s3_path: "s3://my-bucket"

# Specific folder in bucket
s3_path: "s3://my-bucket/documents"

# Nested folders
s3_path: "s3://my-bucket/data/2024/reports"

# With trailing slash (optional)
s3_path: "s3://my-bucket/documents/"
```

## File Extension Filtering

### Include All Files

```yaml
s3_crawler:
  s3_path: "s3://my-bucket"
  extensions: ["*"]
```

### Specific File Types

```yaml
s3_crawler:
  s3_path: "s3://my-bucket"
  extensions: [".pdf", ".docx"]
```

### Common Extensions

```yaml
# Documents
extensions: [".pdf", ".doc", ".docx", ".ppt", ".pptx"]

# Data files
extensions: [".csv", ".xls", ".xlsx", ".json", ".xml"]

# Text files
extensions: [".txt", ".md", ".rst", ".html"]

# Media files (with transcription support)
extensions: [".mp3", ".wav", ".mp4", ".webm"]

# Multiple types
extensions: [".pdf", ".docx", ".xlsx", ".csv", ".txt"]
```

## Metadata File Format

Optional CSV metadata file in S3 allows you to attach custom metadata to specific files. The first column must be the S3 object key, and additional columns become metadata fields.

### Metadata CSV Example

```csv
filename,department,author,classification,review_status
documents/report-q4.pdf,Finance,John Smith,Confidential,Approved
documents/report-q3.pdf,Finance,Jane Doe,Confidential,Approved
guides/employee-handbook.pdf,HR,Alice Johnson,Public,Published
guides/onboarding.pdf,HR,Bob Lee,Internal,Draft
```

### File Location and Usage

1. **Upload to S3**:
   - Upload metadata.csv to your S3 bucket
   - Example: `s3://my-bucket/metadata.csv`

2. **Configure Crawler**:
   ```yaml
   s3_crawler:
     s3_path: "s3://my-bucket/documents"
     metadata_file: "metadata.csv"
   ```
   Note: Specify only the filename if in bucket root, or relative path if in a folder

3. **Metadata Attachment**:
   - Crawler downloads metadata.csv
   - Matches files by their S3 object key
   - Attaches custom metadata to indexed documents
   - Skips the metadata file itself during crawling

### Metadata Structure

- **First Column**: `filename` - The S3 object key (path from bucket root)
- **Additional Columns**: Any custom metadata fields you want to attach
- **Format**: CSV (comma-separated)

### Example: Metadata with Nested Folders

S3 bucket structure:
```
s3://my-bucket/
  ├── documents/
  │   ├── reports/
  │   │   ├── Q4-summary.pdf
  │   │   └── Q3-summary.pdf
  │   ├── guides/
  │   │   └── handbook.pdf
  │   └── metadata.csv
```

metadata.csv content:
```csv
filename,source_system,date_created,owner,version
reports/Q4-summary.pdf,SAP,2024-11-01,Finance Team,2.1
reports/Q3-summary.pdf,SAP,2024-08-01,Finance Team,2.0
guides/handbook.pdf,Confluence,2024-06-15,HR Team,1.3
```

Configuration:
```yaml
s3_crawler:
  s3_path: "s3://my-bucket/documents"
  metadata_file: "metadata.csv"
```

## S3-Compatible Services

The crawler works with S3-compatible storage services like MinIO, DigitalOcean Spaces, and others.

### MinIO Configuration

```yaml
s3_crawler:
  s3_path: "s3://my-bucket/documents"

  aws_access_key_id: "${MINIO_ACCESS_KEY}"
  aws_secret_access_key: "${MINIO_SECRET_KEY}"

  # MinIO endpoint
  endpoint_url: "https://minio.example.com:9000"

vectara:
  # MinIO uses self-signed certs often
  ssl_verify: false  # Or provide custom CA certificate
```

### DigitalOcean Spaces Configuration

```yaml
s3_crawler:
  s3_path: "s3://my-space-name/documents"

  aws_access_key_id: "${DO_SPACES_KEY}"
  aws_secret_access_key: "${DO_SPACES_SECRET}"

  # DigitalOcean Spaces endpoint (regional)
  endpoint_url: "https://nyc3.digitaloceanspaces.com"
```

### Custom S3-Compatible Service

For any S3-compatible service:

```yaml
s3_crawler:
  s3_path: "s3://bucket/prefix"

  aws_access_key_id: "${ACCESS_KEY}"
  aws_secret_access_key: "${SECRET_KEY}"

  endpoint_url: "https://s3.example.com"
```

## Audio and Video File Support

The S3 crawler can index media files with automatic transcription.

### Supported Audio Formats

- MP3, WAV, FLAC, AAC, OGG, WMA, M4A, OPUS

### Supported Video Formats

- MP4, AVI, MOV, WebM, MKV, WMV, FLV, MPEG/MPG, M4V, 3GP, F4V

### Media Configuration

```yaml
s3_crawler:
  s3_path: "s3://my-bucket/media"
  extensions: [".mp3", ".mp4"]

doc_processing:
  # Audio/video transcription (requires appropriate model)
  model_config:
    text:
      provider: openai
      model_name: "whisper-1"
```

When enabled:
- Media files are transcribed to text
- Transcriptions are indexed with media metadata
- Original file S3 metadata preserved
- Searchable through transcribed text content

## Parallel Processing with Ray

Process large S3 buckets efficiently using distributed Ray workers.

### Sequential Processing (Default)

For small numbers of files (< 100):

```yaml
s3_crawler:
  ray_workers: 0  # Sequential processing
  num_per_second: 10
```

### Parallel Processing

Enable Ray workers for large buckets:

```yaml
s3_crawler:
  ray_workers: 4  # Use 4 parallel workers
  num_per_second: 10
```

### Auto-Detect CPU Count

Let the crawler auto-detect available CPUs:

```yaml
s3_crawler:
  ray_workers: -1  # Use all available CPU cores
  num_per_second: 10
```

### How Parallel Processing Works

1. **Worker Pool**: Creates N worker processes (each with own S3 client)
2. **File Distribution**: Distributes S3 files across workers
3. **Parallel Download**: Files downloaded in parallel
4. **Rate Limiting**: `num_per_second` limit applies globally
5. **Indexing**: Files indexed as downloaded

## Rate Limiting

Control the speed of file downloads and indexing:

```yaml
s3_crawler:
  num_per_second: 10  # Download/index 10 files per second
```

### Guidelines

- **Default (10 files/second)**: Balanced for most S3 buckets
- **Conservative (1-3 files/second)**: For API rate limit constraints
- **Aggressive (20+ files/second)**: For high-speed processing (with caution)

### With Parallel Workers

Rate limiting applies globally even with multiple workers:

```yaml
s3_crawler:
  ray_workers: 4          # 4 parallel workers
  num_per_second: 20      # Total of 20 files/second across all workers
                          # ~5 files/second per worker
```

## Automatic Metadata

Every S3 file indexed includes automatic metadata:

```yaml
source: "S3"              # Source identifier
title: "filename.pdf"     # S3 object key
url: "s3://bucket/key"    # S3 URI to file
```

Plus any custom metadata from metadata CSV file.

## Bucket Permissions

### Verification Script

Test S3 bucket access:

```bash
#!/bin/bash
export AWS_ACCESS_KEY_ID="your-key"
export AWS_SECRET_ACCESS_KEY="your-secret"

# List bucket contents
aws s3 ls s3://my-bucket/documents/ --recursive | head -20

# Download test file
aws s3 cp s3://my-bucket/documents/test.pdf ./ --sse AES256
```

### Common Permission Issues

**Error**: `AccessDenied: User is not authorized to perform: s3:ListBucket`

**Solution**: Add `s3:ListBucket` permission to IAM policy

**Error**: `NoSuchKey: The specified key does not exist`

**Solution**: Verify S3 path is correct and files exist

**Error**: `InvalidAccessKeyId: The AWS Access Key Id you provided does not exist`

**Solution**: Verify credentials are correct and haven't expired

## Performance Optimization

### For Large Buckets (1000+ files)

1. **Enable Parallel Processing**:
   ```yaml
   s3_crawler:
     ray_workers: -1  # Auto-detect
     num_per_second: 20
   ```

2. **Filter by Extension**:
   ```yaml
   s3_crawler:
     extensions: [".pdf", ".docx"]  # Only needed types
   ```

3. **Use Specific Prefix**:
   ```yaml
   s3_crawler:
     s3_path: "s3://bucket/2024"  # Instead of root
   ```

### Memory Optimization

For memory-constrained environments:

```yaml
s3_crawler:
  ray_workers: 2          # Fewer workers
  num_per_second: 5       # Slower rate

doc_processing:
  process_locally: true
  doc_parser: unstructured
```

### Chunk Size Optimization

Adjust chunking for your content:

```yaml
vectara:
  chunking_strategy: sentence  # Better for varied content
  chunk_size: 512              # Balanced (adjust 256-1024)
```

## Example Configurations

### Simple PDF Crawl

```yaml
# config/s3-pdfs.yaml
vectara:
  endpoint: api.vectara.io
  corpus_key: s3-pdfs

crawling:
  crawler_type: s3

s3_crawler:
  s3_path: "s3://my-bucket/reports"
  extensions: [".pdf"]
  aws_access_key_id: "${AWS_ACCESS_KEY_ID}"
  aws_secret_access_key: "${AWS_SECRET_ACCESS_KEY}"

metadata:
  source: s3-reports
```

### Mixed Document Types with Metadata

```yaml
# config/s3-documents.yaml
vectara:
  endpoint: api.vectara.io
  corpus_key: s3-documents

crawling:
  crawler_type: s3

s3_crawler:
  s3_path: "s3://company-docs/knowledge-base"
  extensions: [".pdf", ".docx", ".xlsx", ".csv"]
  metadata_file: "metadata.csv"
  source: "knowledge-base"
  aws_access_key_id: "${AWS_ACCESS_KEY_ID}"
  aws_secret_access_key: "${AWS_SECRET_ACCESS_KEY}"
  ray_workers: 4
  num_per_second: 10

metadata:
  source: s3
  environment: production
```

With metadata.csv in S3:
```csv
filename,department,owner,review_status
policies/security.pdf,IT,Alice,Approved
procedures/hiring.docx,HR,Bob,Published
templates/invoice.xlsx,Finance,Charlie,Published
```

### Large-Scale Parallel Processing

```yaml
# config/s3-large.yaml
vectara:
  endpoint: api.vectara.io
  corpus_key: s3-archive
  reindex: false

crawling:
  crawler_type: s3

s3_crawler:
  s3_path: "s3://data-lake/archive"
  extensions: ["*"]
  source: "data-lake"
  aws_access_key_id: "${AWS_ACCESS_KEY_ID}"
  aws_secret_access_key: "${AWS_SECRET_ACCESS_KEY}"
  ray_workers: -1              # Use all CPUs
  num_per_second: 20           # Higher rate

doc_processing:
  process_locally: true
  doc_parser: unstructured

metadata:
  source: s3
  batch_size: large
```

### MinIO Instance

```yaml
# config/s3-minio.yaml
vectara:
  endpoint: api.vectara.io
  corpus_key: minio-docs
  ssl_verify: false

crawling:
  crawler_type: s3

s3_crawler:
  s3_path: "s3://documents/reports"
  extensions: [".pdf", ".docx"]
  aws_access_key_id: "${MINIO_ACCESS_KEY}"
  aws_secret_access_key: "${MINIO_SECRET_KEY}"
  endpoint_url: "https://minio.internal:9000"
  num_per_second: 5

metadata:
  source: minio
  environment: internal
```

### Media Files with Transcription

```yaml
# config/s3-media.yaml
vectara:
  endpoint: api.vectara.io
  corpus_key: media-transcripts

crawling:
  crawler_type: s3

s3_crawler:
  s3_path: "s3://media-archive/recordings"
  extensions: [".mp3", ".mp4"]
  source: "media-library"
  aws_access_key_id: "${AWS_ACCESS_KEY_ID}"
  aws_secret_access_key: "${AWS_SECRET_ACCESS_KEY}"
  ray_workers: 2

doc_processing:
  model_config:
    text:
      provider: openai
      model_name: "whisper-1"

metadata:
  content_type: media
```

## Troubleshooting

### Authentication Failed

**Error**: `InvalidAccessKeyId: The AWS Access Key Id you provided does not exist`

**Solutions**:
1. Verify credentials are correct:
   ```bash
   echo $AWS_ACCESS_KEY_ID
   echo $AWS_SECRET_ACCESS_KEY
   ```

2. Check credentials haven't expired

3. Verify environment variables are set:
   ```bash
   export AWS_ACCESS_KEY_ID="your-key"
   export AWS_SECRET_ACCESS_KEY="your-secret"
   ```

### Bucket Not Found

**Error**: `NoSuchBucket: The specified bucket does not exist`

**Solutions**:
1. Verify bucket name in S3 path:
   ```yaml
   s3_path: "s3://correct-bucket-name"  # Exact spelling
   ```

2. Verify bucket exists in your AWS account:
   ```bash
   aws s3 ls | grep bucket-name
   ```

3. Check bucket is in correct region

### Access Denied

**Error**: `AccessDenied: User is not authorized to perform: s3:ListBucket`

**Solutions**:
1. Verify IAM permissions include `s3:ListBucket` and `s3:GetObject`

2. Apply correct policy:
   ```json
   {
     "Effect": "Allow",
     "Action": ["s3:ListBucket", "s3:GetObject"],
     "Resource": ["arn:aws:s3:::bucket-name", "arn:aws:s3:::bucket-name/*"]
   }
   ```

3. Check IAM user/role is correct:
   ```bash
   aws iam get-user
   ```

### SSL Certificate Errors

**Error**: `SSL: CERTIFICATE_VERIFY_FAILED` (for S3-compatible services)

**Solutions**:
1. Disable verification (development only):
   ```yaml
   vectara:
     ssl_verify: false
   ```

2. Provide custom certificate:
   ```yaml
   vectara:
     ssl_verify: "/path/to/ca-bundle.crt"
   ```

3. Add certificate to system trust store

### Path Not Found

**Error**: `The specified key does not exist` or no files processed

**Solutions**:
1. Verify S3 path exists and has files:
   ```bash
   aws s3 ls s3://bucket-name/path/
   ```

2. Check file extensions match configuration:
   ```yaml
   extensions: ["*"]  # Start with all files
   ```

3. Verify prefix is correct:
   ```bash
   aws s3 ls s3://bucket-name/prefix/ --recursive | head -5
   ```

### No Credentials Error

**Error**: `NoCredentialsError: Unable to locate credentials`

**Solutions**:
1. Set environment variables:
   ```bash
   export AWS_ACCESS_KEY_ID="your-key"
   export AWS_SECRET_ACCESS_KEY="your-secret"
   bash run.sh config/s3.yaml default
   ```

2. Or use AWS credential file:
   ```bash
   mkdir -p ~/.aws
   cat > ~/.aws/credentials << EOF
   [default]
   aws_access_key_id = YOUR_KEY
   aws_secret_access_key = YOUR_SECRET
   EOF
   chmod 600 ~/.aws/credentials
   ```

### Ray Worker Errors

**Error**: `Failed to initialize Ray workers`

**Solutions**:
1. Reduce worker count:
   ```yaml
   s3_crawler:
     ray_workers: 2  # Instead of auto-detect
   ```

2. Check available system resources:
   ```bash
   free -h        # Memory
   nproc          # CPU cores
   ```

3. Enable local processing:
   ```yaml
   doc_processing:
     process_locally: true
   ```

### Out of Memory

**Error**: `MemoryError` or container killed

**Solutions**:
1. Reduce parallel workers:
   ```yaml
   s3_crawler:
     ray_workers: 1  # Reduce parallelism
   ```

2. Reduce rate limiting:
   ```yaml
   s3_crawler:
     num_per_second: 5  # Slower processing
   ```

3. Skip expensive processing:
   ```yaml
   doc_processing:
     do_ocr: false
     summarize_images: false
   ```

### Metadata File Issues

**Error**: `Metadata file not found` or `Failed to parse CSV`

**Solutions**:
1. Verify metadata file exists in S3:
   ```bash
   aws s3 ls s3://bucket-name/metadata.csv
   ```

2. Check file path is correct in configuration

3. Verify CSV format is valid (use online CSV validator)

4. Ensure first column is named `filename` (exact case)

## Best Practices

### 1. Always Use Environment Variables

```bash
# DO: Use environment variables
export AWS_ACCESS_KEY_ID="AKIAIOSFODNN7EXAMPLE"
export AWS_SECRET_ACCESS_KEY="wJalrXUtnFEMI/..."
bash run.sh config/s3.yaml default

# DO NOT: Hard-code in config
s3_crawler:
  aws_access_key_id: "AKIAIOSFODNN7EXAMPLE"  # WRONG!
  aws_secret_access_key: "wJalrXUtnFEMI/..."  # WRONG!
```

### 2. Secure Credential Management

```bash
# Use AWS credential file (more secure than env vars for persistent use)
~/.aws/credentials

# Or use AWS IAM roles in production (no explicit credentials needed)
# Works automatically if crawler runs on EC2, ECS, Lambda, etc.
```

### 3. Use Specific S3 Prefixes

```yaml
# Good: Specific folder
s3_path: "s3://bucket/documents/2024"

# Less efficient: Bucket root
s3_path: "s3://bucket"
```

### 4. Filter by File Extension

```yaml
# Good: Only needed types
extensions: [".pdf", ".docx"]

# Less efficient: All files
extensions: ["*"]
```

### 5. Start Small and Scale Up

Test with small subset first:

```yaml
s3_crawler:
  s3_path: "s3://bucket/test-folder"  # Test with one folder
  extensions: [".pdf"]                 # Single type
  ray_workers: 0                       # Sequential
```

Then expand configuration for production.

### 6. Use Metadata CSV for Organization

```csv
filename,department,owner,classification
docs/policy.pdf,HR,alice@company.com,Public
docs/budget.xlsx,Finance,bob@company.com,Confidential
```

### 7. Monitor Crawls

```bash
# Watch logs in real-time
docker logs -f vingest

# Check for errors
docker logs vingest | grep -i error

# Verify files indexed
docker logs vingest | grep "Indexing"
```

### 8. Version Your Configurations

```yaml
metadata:
  version: "1.0"
  created_date: "2024-11-18"
  notes: "Production S3 crawler configuration"
  last_updated: "2024-11-18"
```

### 9. Use Appropriate Rate Limiting

```yaml
# Conservative (avoid rate limits)
num_per_second: 5

# Balanced (recommended)
num_per_second: 10

# Aggressive (use with caution)
num_per_second: 20
```

### 10. Enable Parallel Processing for Large Buckets

```yaml
# Small bucket (< 100 files)
ray_workers: 0

# Medium bucket (100-1000 files)
ray_workers: 4

# Large bucket (> 1000 files)
ray_workers: -1  # Auto-detect
```

## Running the Crawler

### Set Environment Variables

```bash
export AWS_ACCESS_KEY_ID="your-key"
export AWS_SECRET_ACCESS_KEY="your-secret"
export VECTARA_API_KEY="your-vectara-key"
```

Or use AWS credential file:

```bash
# AWS SDK automatically reads from ~/.aws/credentials
# No need to set AWS_ACCESS_KEY_ID or AWS_SECRET_ACCESS_KEY
```

### Run Single Crawl

```bash
bash run.sh config/s3.yaml default
```

### Monitor Progress

```bash
docker logs -f vingest
```

### Scheduling Regular Crawls

Use cron for scheduled indexing:

```bash
# Daily at 2 AM
0 2 * * * cd /path/to/vectara-ingest && \
  export AWS_ACCESS_KEY_ID="key" && \
  export AWS_SECRET_ACCESS_KEY="secret" && \
  bash run.sh config/s3.yaml default

# Weekly on Sunday
0 2 * * 0 cd /path/to/vectara-ingest && \
  export AWS_ACCESS_KEY_ID="key" && \
  export AWS_SECRET_ACCESS_KEY="secret" && \
  bash run.sh config/s3.yaml default

# Every 6 hours
0 */6 * * * cd /path/to/vectara-ingest && \
  export AWS_ACCESS_KEY_ID="key" && \
  export AWS_SECRET_ACCESS_KEY="secret" && \
  bash run.sh config/s3.yaml default
```

Or store credentials in `.env` file:

```bash
# .env
AWS_ACCESS_KEY_ID="your-key"
AWS_SECRET_ACCESS_KEY="your-secret"

# Cron job
0 2 * * * cd /path/to/vectara-ingest && source .env && \
  bash run.sh config/s3.yaml default
```

## Related Documentation

- [Base Configuration](../configuration-base.md) - Common settings for all crawlers
- [Crawlers Overview](index.md) - Other available crawlers
- [Document Processing](../features/document-processing.md) - Advanced document processing options
- [Deployment](../deployment/docker.md) - Running in production
- [Getting Started](../getting-started.md) - Initial setup guide

## AWS Resources

- [AWS S3 Documentation](https://docs.aws.amazon.com/s3/)
- [AWS IAM Policies for S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html)
- [AWS Access Keys](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html)
- [AWS CLI S3 Guide](https://docs.aws.amazon.com/cli/latest/userguide/cli-services-s3.html)
- [S3-Compatible Storage Services](https://aws.amazon.com/s3/storage-classes/s3-compatible/)

## Common Workflows

### Workflow 1: Index All Documents in a Bucket

```yaml
# config/s3-all-docs.yaml
vectara:
  endpoint: api.vectara.io
  corpus_key: s3-all-documents

crawling:
  crawler_type: s3

s3_crawler:
  s3_path: "s3://company-documents"
  extensions: [".pdf", ".docx", ".xlsx"]
  aws_access_key_id: "${AWS_ACCESS_KEY_ID}"
  aws_secret_access_key: "${AWS_SECRET_ACCESS_KEY}"
  ray_workers: -1
  num_per_second: 15

metadata:
  source: s3
  environment: production
```

### Workflow 2: Department-Specific Crawls

```yaml
# config/s3-finance.yaml
vectara:
  endpoint: api.vectara.io
  corpus_key: finance-documents

crawling:
  crawler_type: s3

s3_crawler:
  s3_path: "s3://company-documents/finance"
  extensions: [".xlsx", ".csv", ".pdf"]
  metadata_file: "finance-metadata.csv"
  aws_access_key_id: "${AWS_ACCESS_KEY_ID}"
  aws_secret_access_key: "${AWS_SECRET_ACCESS_KEY}"

metadata:
  source: s3
  department: finance
```

### Workflow 3: Archive Old Documents

```yaml
# config/s3-archive.yaml
vectara:
  endpoint: api.vectara.io
  corpus_key: s3-archive
  reindex: true

crawling:
  crawler_type: s3

s3_crawler:
  s3_path: "s3://archive-bucket/legacy"
  extensions: ["*"]
  aws_access_key_id: "${AWS_ACCESS_KEY_ID}"
  aws_secret_access_key: "${AWS_SECRET_ACCESS_KEY}"
  ray_workers: -1

metadata:
  source: s3
  collection_type: archive
```

## Performance Benchmarks

Typical performance on modern hardware (4 CPU cores, 8GB RAM):

- **Small files (< 1MB)**: 50-100 files/minute (sequential), 200-400 files/minute (parallel)
- **Medium files (1-10MB)**: 20-50 files/minute (sequential), 60-150 files/minute (parallel)
- **Large files (> 10MB)**: 5-20 files/minute (sequential), 15-60 files/minute (parallel)
- **Mixed content**: 20-50 files/minute (sequential), 80-200 files/minute (parallel)

With parallel processing (-1 workers):
- Increases throughput by 3-4x depending on file types
- Uses additional CPU and memory

For 10,000 files:
- Sequential: ~3-6 hours
- Parallel (4 workers): ~45-90 minutes

Actual performance depends on:
- File size and complexity
- S3 bucket region and network latency
- Processor speed
- Available memory
- Document processing options (OCR, table extraction, etc.)
- Vectara API latency

## FAQ

**Q: Can I use IAM roles instead of access keys?**
A: Yes! If the crawler runs on EC2, ECS, Lambda, or other AWS services, it can use IAM roles automatically without explicit credentials.

**Q: Do I need credentials for public S3 buckets?**
A: No, public buckets don't require authentication, but credentials are still needed for `ListBucket` permission.

**Q: What happens if a file fails to index?**
A: The crawler logs the error and continues with the next file. Failed files are not retried automatically.

**Q: Can I filter files by name pattern?**
A: Not directly in the crawler config, but use specific S3 prefixes: `s3://bucket/2024-reports`

**Q: How do I update existing documents in Vectara?**
A: Use `reindex: false` and rerun the crawler. Modified files will be updated.

**Q: Can I delete documents that were removed from S3?**
A: Documents remain indexed. Use `reindex: true` to replace all indexed content on next run.

**Q: What's the maximum number of files I can crawl?**
A: No hard limit. Tested with 100,000+ files. Adjust workers and rate limiting as needed.

**Q: How do I handle very large files (> 100MB)?**
A: Large files are automatically split into chunks. Adjust chunk size if needed.

**Q: Can I use multiple S3 crawlers in the same corpus?**
A: Yes, run multiple crawlers with the same corpus_key to accumulate documents.

**Q: How do I troubleshoot SSL certificate errors?**
A: For development: set `ssl_verify: false`. For production: provide custom CA certificate path.

**Q: Can I crawl multiple buckets?**
A: Run separate crawler instances for each bucket, or create multiple configurations.
